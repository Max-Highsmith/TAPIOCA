{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Transformer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch import nn as nn\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import preprocessing\n",
    "from torch.nn import TransformerEncoderLayer, TransformerEncoder\n",
    "from torch.nn import functional as F\n",
    "from Data.Drosophilla.FlyDataMod import FlyDataModule\n",
    "import yaml\n",
    "import os\n",
    "from pytorch_lightning.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = FlyDataModule(cell_line=\"S2\",\n",
    "                  data_win_radius=5,\n",
    "                  batch_size=2,\n",
    "                  label_type=\"gamma\",\n",
    "                  label_val=10)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=11):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:,:-1]\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "def weighted_mse(output, target):\n",
    "    alpha = 11\n",
    "    N     = output.shape[1]\n",
    "    wmse  = torch.mean(1/N * torch.sum((target-output)**2* (alpha-target)/alpha, dim=1))\n",
    "    return wmse\n",
    "\n",
    "class  TransformerModule(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                ntoken,\n",
    "                ninp,\n",
    "                nhead,\n",
    "                nhid,\n",
    "                nlayers,\n",
    "                dropout=0.5,\n",
    "                optimi=\"Adam\",\n",
    "                lr=.01,\n",
    "                loss_type=\"weighted\"):\n",
    "        print(\"init\")\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ntoken    = ntoken\n",
    "        self.ninp      = int(ninp)\n",
    "        self.nhead     = nhead\n",
    "        self.nhid      = nhid\n",
    "        self.nlayers   = nlayers\n",
    "        self.dropout   = dropout\n",
    "        self.optimi    = optimi\n",
    "        self.lr        = lr\n",
    "        self.loss_type = loss_type\n",
    "        \n",
    "        self.pos_enc   = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layer  = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, nlayers)\n",
    "        self.decoder   = nn.Linear(ninp, ntoken)\n",
    "        self.src_mask  = None\n",
    "        self.init_weights()\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0,1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask ==1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, has_mask=True):\n",
    "        BATCH_SIZE = src.shape[0]\n",
    "        SEQ_LEN    = src.shape[1]\n",
    "        EMBED_DIM  = src.shape[2]\n",
    "        src        = src.permute(1, 0, 2)\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                mask = mask.float()\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "        src    = src * math.sqrt(self.ninp)\n",
    "        src    = self.pos_enc(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        output = output.permute(1,0,2)\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        feature, label = batch\n",
    "        feature        = feature.float()\n",
    "        label          = label.float()\n",
    "        output         = self.forward(feature)\n",
    "        #loss           = F.mse_loss(output, label)\n",
    "        if self.loss_type ==\"weighted\":\n",
    "            loss           = weighted_mse(output, label)\n",
    "        else:\n",
    "            loss       = F.mse_loss(output, label)\n",
    "        self.log(\"train weighted mse loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_indx):\n",
    "        feature, label = batch\n",
    "        feature        = feature.float()\n",
    "        output         = self.forward(feature)\n",
    "        loss           = weighted_mse(output, label)\n",
    "        self.log(\"val weighted mse loss\", loss)\n",
    "        return loss\n",
    "        \n",
    "    def test_step(self, batch, batch_indx):\n",
    "        feature, label = batch\n",
    "        feature        = feature.float()\n",
    "        output         = self.forward(feature)\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.plot(label[0,:,0].cpu(), label=\"label\")\n",
    "        ax.plot(output[0,:,0].cpu(),  label=\"output\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        #optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        if self.optimi == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        if self.optimi == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        if self.optimi == \"RMSprop\":\n",
    "            optimizer = torch.optim.RMSprop(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model   = TransformerModule(\n",
    "                ntoken=1,\n",
    "                ninp=29,\n",
    "                nhead=1,\n",
    "                nhid=2048,\n",
    "                nlayers=1,\n",
    "                dropout=0,\n",
    "                optimi=\"Adam\",\n",
    "                lr=.001)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,\n",
    "                    max_epochs=200)\n",
    "\n",
    "trainer.fit(model, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a few helper functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "def getModelPredictions(model,\n",
    "                   dm,\n",
    "                   tvt):\n",
    "    if tvt==\"test\":\n",
    "        dataloader = dm.test_dataloader()\n",
    "    if tvt==\"train\":\n",
    "        dataloader = dm.train_dataloader()\n",
    "    if tvt==\"val\":\n",
    "        dataloader =dm.val_dataloader()\n",
    "        \n",
    "    full_label_vec  = []\n",
    "    full_output_vec = []\n",
    "    for b, batch in enumerate(dataloader):\n",
    "        feature, label = batch\n",
    "        feature = feature.to('cuda:0').float()\n",
    "        label   = label.to('cuda:0').float()\n",
    "        output  = model(feature)\n",
    "        label   = label.squeeze()\n",
    "        output  = output.squeeze()\n",
    "        full_label_vec.append(label[int(len(label)/2)].item())\n",
    "        full_output_vec.append(output[int(len(output)/2)].item())\n",
    "\n",
    "    return full_label_vec, full_output_vec\n",
    "\n",
    "def getModelMetrics(model,\n",
    "                   dm,\n",
    "                   tvt):\n",
    "    label_vec, output_vec = getModelPredictions(model,\n",
    "                                               dm,\n",
    "                                               tvt)\n",
    "    scores             = {}\n",
    "    scores['mse']      = mean_squared_error(label_vec, output_vec)\n",
    "    scores['mae']      = mean_absolute_error(label_vec, output_vec)\n",
    "    scores['r2']       = r2_score(label_vec, output_vec)\n",
    "    scores['pearson']  = pearsonr(label_vec, output_vec)[0]\n",
    "    scores['spearman'] = spearmanr(label_vec, output_vec)[0]\n",
    "    return scores\n",
    "\n",
    "def createPlot(model,\n",
    "                dm,\n",
    "                tvt,\n",
    "               fig_name,\n",
    "               start=0,\n",
    "              end=200):\n",
    "    fig, ax = plt.subplots(1, figsize=(20,7))\n",
    "    label_vec, output_vec = getModelPredictions(model, dm, tvt)\n",
    "    ax.plot(label_vec[start:end],  label=\"label\",      color=\"crimson\")\n",
    "    ax.plot(output_vec[start:end], label=\"prediction\", color=\"silver\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_ylabel(dm.label_type)\n",
    "    ax.set_xlabel(tvt+\"data\")\n",
    "    plt.legend()\n",
    "    plt.savefig(fig_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = FlyDataModule(cell_line=\"S2\",\n",
    "                  data_win_radius=5,\n",
    "                  batch_size=1,\n",
    "                  label_type=\"gamma\",\n",
    "                  label_val=10)\n",
    "dm.setup()\n",
    "print(getModelMetrics(model, dm, 'val'))\n",
    "createPlot(model, dm, \"val\", \"val.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulled from https://github.com/optuna/optuna/issues/1186\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "from pytorch_lightning.loggers import LightningLoggerBase\n",
    "\n",
    "class DictLogger(LightningLoggerBase):\n",
    "    \"\"\"PyTorch Lightning `dict` logger.\"\"\"\n",
    "\n",
    "    def __init__(self, version, root_dir):\n",
    "        super(DictLogger, self).__init__()\n",
    "        self.metrics = []\n",
    "        self._version = version\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def log_metrics(self, metrics, step=None):\n",
    "        self.metrics.append(metrics)\n",
    "\n",
    "    @property\n",
    "    def version(self):\n",
    "        return self._version\n",
    "\n",
    "    @property\n",
    "    def experiment(self):\n",
    "        \"\"\"Return the experiment object associated with this logger.\"\"\"\n",
    "\n",
    "    def log_hyperparams(self, params):\n",
    "        \"\"\"\n",
    "        Record hyperparameters.\n",
    "        Args:\n",
    "            params: :class:`~argparse.Namespace` containing the hyperparameters\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(self.root_dir):\n",
    "            os.mkdir(self.root_dir)\n",
    "        if not os.path.isdir(self.root_dir+\"/optuna\"):\n",
    "            os.mkdir(self.root_dir+\"/optuna\")\n",
    "        dirr = self.root_dir+'/optuna/version_'+str(self._version)\n",
    "        if not os.path.isdir(dirr):\n",
    "            os.mkdir(self.root_dir+'/optuna/version_'+str(self._version))\n",
    "        with open(dirr+\"/hparams.yaml\", 'w') as outfile:\n",
    "            print(\"logging them hyperparams:\"+str(self.root_dir))\n",
    "            yaml.dump(params, outfile)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"Return the experiment name.\"\"\"\n",
    "        return 'optuna'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experiment 1\n",
    "# Determine Hyper Parameters for Transformer\n",
    "import optuna\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "def objective(trial):\n",
    "    rootdir = \"Experiments/Transformer_Hyperparameter_Tuning\"\n",
    "    if not os.path.isdir(rootdir):\n",
    "        os.mkdir(rootdir)\n",
    "    logger = DictLogger(trial.number,\n",
    "                       rootdir)\n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        gpus=1,\n",
    "        max_epochs=200,\n",
    "        default_root_dir=rootdir\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #hyper params\n",
    "    lr      = trial.suggest_categorical(\"lr\", [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n",
    "    dropout    = trial.suggest_categorical(\"dropout\",[0, 0.1, 0.2, 0.3])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [1,4,16,64])\n",
    "    optimi     = trial.suggest_categorical(\"optimi\", [\"Adam\", \"SGD\"])\n",
    "    \n",
    "    dm = FlyDataModule(cell_line=\"S2\",\n",
    "                  data_win_radius=5,\n",
    "                  batch_size=batch_size,\n",
    "                  label_type=\"gamma\",\n",
    "                  label_val=10)\n",
    "    dm.setup()\n",
    "    \n",
    "    model   = TransformerModule(\n",
    "                ntoken=1,\n",
    "                ninp=29,\n",
    "                nhead=1,\n",
    "                nhid=2048,\n",
    "                nlayers=6,\n",
    "                dropout=dropout,\n",
    "                optimi=optimi,\n",
    "                lr=lr)\n",
    "    \n",
    "    trainer.fit(model, dm)\n",
    "    return logger.metrics[-1]['val weighted mse loss']\n",
    "\n",
    "        \n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "print(\"Best Trial\")\n",
    "trial = study.best_trial\n",
    "print(trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2\n",
    "# Vary number of layers\n",
    "import yaml\n",
    "import os\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "best_params = yaml.load(open(\"Experiments/Transformer_Hyperparameter_Tuning/optuna/version_4/hparams.yaml\"))\n",
    "print(best_params)\n",
    "\n",
    "dm = FlyDataModule(cell_line=\"S2\",\n",
    "                  data_win_radius=5,\n",
    "                  batch_size=1,\n",
    "                  label_type='gamma')\n",
    "dm.setup()\n",
    "\n",
    "for num_layers in reversed(range(1,10)):\n",
    "    early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val weighted mse loss\",\n",
    "    min_delta=0.00,\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode='min')\n",
    "    \n",
    "    root_dir = \"Experiments/Transformer_Num_Layers\"\n",
    "    if not os.path.isdir(root_dir):\n",
    "        os.mkdir(root_dir)\n",
    "    \n",
    "    logger = DictLogger(num_layers,\n",
    "                       root_dir)\n",
    "    \n",
    "    print(\"Training:\"+str(num_layers))\n",
    "    model = TransformerModule(\n",
    "        ntoken=best_params['ntoken'],\n",
    "        ninp=best_params['ninp'],\n",
    "        nhid=best_params['nhid'],\n",
    "        nhead=best_params['nhead'],\n",
    "        nlayers=num_layers,\n",
    "        dropout=best_params['dropout'],\n",
    "        optimi=best_params['optimi'],\n",
    "        lr=best_params['lr'])\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        gpus=1,\n",
    "        max_epochs=200,\n",
    "        default_root_dir=root_dir,\n",
    "        callbacks=[early_stop_callback])\n",
    "    \n",
    "    trainer.fit(model, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm         = FlyDataModule(cell_line=\"S2\",\n",
    "                  data_win_radius=5,\n",
    "                  batch_size=1,\n",
    "                  label_type=\"gamma\",\n",
    "                  label_val=10)\n",
    "dm.setup()\n",
    "best_weights = \"Experiments/Transformer_Hyperparameter_Tuning/optuna/version_4/checkpoints/epoch=199-step=195599.ckpt\"\n",
    "best_model   = TransformerModule.load_from_checkpoint(best_weights).to(\"cuda:0\")\n",
    "print(best_model)\n",
    "print(getModelMetrics(best_model, dm, 'test'))\n",
    "createPlot(best_model, dm, \"test\", \"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparm_vals = np.zeros((11,8), dtype='U7')\n",
    "metrics    = ['mse','mae','r2', 'pearson','spearman']\n",
    "\n",
    "labels=list(yaml.load(open(\"Experiments/Transformer_Hyperparameter_Tuning/optuna/version_1/hparams.yaml\")).keys())\n",
    "hparm_vals[0,0]=labels[0]\n",
    "hparm_vals[0,1]=labels[1]\n",
    "hparm_vals[0,2]=labels[7]\n",
    "for j, metric in enumerate(metrics):\n",
    "    hparm_vals[0,3+j]=metric\n",
    "for i in range(0,10):\n",
    "    hpar = list(yaml.load(open(\"Experiments/Transformer_Hyperparameter_Tuning/optuna/version_\"+str(i)+\"/hparams.yaml\")).values())\n",
    "    hparm_vals[i+1,0]=hpar[0]\n",
    "    hparm_vals[i+1,1]=hpar[1]\n",
    "    hparm_vals[i+1,2]=hpar[7]\n",
    "    layer_weights = glob.glob(\"Experiments/Transformer_Hyperparameter_Tuning/optuna/version_\"+str(i)+\"/checkpoints/*\")[0]\n",
    "    layer_model   = TransformerModule.load_from_checkpoint(layer_weights).to(\"cuda:0\")\n",
    "    results= getModelMetrics(layer_model,\n",
    "                   dm,\n",
    "                   'val')\n",
    "    for j, metric in enumerate(metrics):\n",
    "        hparm_vals[i+1, j+3]= results[metric]\n",
    "print(hparm_vals)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.axis('off')\n",
    "#for loc in ['top','bottom','left','right']:\n",
    "#    ax.spines[loc].set_visible(False)\n",
    "table = ax.table(hparm_vals)\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1.5, 1.5)\n",
    "plt.show()\n",
    "    #layer_weights = glob.glob(\"Experiments/Transformer_Hyperparameter_Tuning/optuna/version_\"+str(i)+\"checkpoints/*\")[0]\n",
    "    #layer_model   = TransformerModule.load_from_checkpoint(layer_weights).to(\"cuda:0\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view \n",
    "import glob\n",
    "dm         = FlyDataModule(cell_line=\"S2\",\n",
    "                  data_win_radius=5,\n",
    "                  batch_size=1,\n",
    "                  label_type=\"gamma\",\n",
    "                  label_val=10)\n",
    "dm.setup()\n",
    "\n",
    "train_metrics = {}\n",
    "val_metrics   = {}\n",
    "\n",
    "for full_metrics, tvt in zip([train_metrics, val_metrics],['train', 'val']):\n",
    "    for i in range(1,10):\n",
    "        layer_weights = glob.glob(\"Experiments/Transformer_Num_Layers/optuna/version_\"+str(i)+\"/checkpoints/*\")[0]\n",
    "        layer_model   = TransformerModule.load_from_checkpoint(layer_weights).to(\"cuda:0\")\n",
    "        metrics = getModelMetrics(layer_model, dm, 'val')\n",
    "        for key in metrics.keys():\n",
    "            if key not in full_metrics:\n",
    "                full_metrics[key] = []\n",
    "            full_metrics[key].append(metrics[key])\n",
    "\n",
    "for key in metrics.keys():\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.plot(list(range(1, len(train_metrics[key])+1)), train_metrics[key], label=\"train\", color=\"cornflowerblue\")\n",
    "    ax.plot(list(range(1, len(train_metrics[key])+1)), val_metrics[key], label=\"val\", color=\"indigo\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_ylabel(key)\n",
    "    ax.set_xlabel(\"num layers\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "# Vary window size\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "best_params = yaml.load(open(\"Experiments/Transformer_Num_Layers/optuna/version_5/hparams.yaml\"))\n",
    "print(best_params)\n",
    "\n",
    "for data_win_radius in reversed(range(1,6)):\n",
    "    early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val weighted mse loss\",\n",
    "    min_delta=0.00,\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode='min')\n",
    "    \n",
    "    \n",
    "    dm = FlyDataModule(cell_line=\"S2\",\n",
    "                  data_win_radius=data_win_radius,\n",
    "                  batch_size=1,\n",
    "                  label_type='gamma')\n",
    "    dm.setup()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    root_dir = \"Experiments/Transformer_Window_Radius\"\n",
    "    if not os.path.isdir(root_dir):\n",
    "        os.mkdir(root_dir)\n",
    "    \n",
    "    logger = DictLogger(data_win_radius,\n",
    "                       root_dir)\n",
    "    \n",
    "    print(\"Training:\"+str(data_win_radius))\n",
    "    model = TransformerModule(\n",
    "        ntoken=best_params['ntoken'],\n",
    "        ninp=best_params['ninp'],\n",
    "        nhid=best_params['nhid'],\n",
    "        nhead=best_params['nhead'],\n",
    "        nlayers=best_params['nlayers'],\n",
    "        dropout=best_params['dropout'],\n",
    "        optimi=best_params['optimi'],\n",
    "        lr=best_params['lr'])\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        gpus=1,\n",
    "        max_epochs=200,\n",
    "        default_root_dir=root_dir,\n",
    "        callbacks=[early_stop_callback])\n",
    "    trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view \n",
    "import glob\n",
    "dm         = FlyDataModule(cell_line=\"S2\",\n",
    "                  data_win_radius=5,\n",
    "                  batch_size=1,\n",
    "                  label_type=\"gamma\",\n",
    "                  label_val=10)\n",
    "dm.setup()\n",
    "\n",
    "train_metrics = {}\n",
    "val_metrics   = {}\n",
    "\n",
    "for full_metrics, tvt in zip([train_metrics, val_metrics],['train', 'val']):\n",
    "    for i in range(1,6):\n",
    "        layer_weights = glob.glob(\"Experiments/Transformer_Window_Radius/optuna/version_\"+str(i)+\"/checkpoints/*\")[0]\n",
    "        layer_model   = TransformerModule.load_from_checkpoint(layer_weights).to(\"cuda:0\")\n",
    "        metrics = getModelMetrics(layer_model, dm, 'val')\n",
    "        for key in metrics.keys():\n",
    "            if key not in full_metrics:\n",
    "                full_metrics[key] = []\n",
    "            full_metrics[key].append(metrics[key])\n",
    "\n",
    "for key in metrics.keys():\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.plot(list(range(1, len(train_metrics[key])+1)), train_metrics[key], label=\"train\", color=\"cornflowerblue\")\n",
    "    ax.plot(list(range(1, len(train_metrics[key])+1)), val_metrics[key], label=\"val\", color=\"indigo\")\n",
    "    ax.set_ylabel(key)\n",
    "    ax.set_xticks([1,2,3,4,5])\n",
    "    ax.set_xlabel(\"Window Radius\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using Transformer across different cell lines\n",
    "cell_lines = ['S2', 'KC', 'BG']\n",
    "for trial, train_line in enumerate(cell_lines):\n",
    "    dm = FlyDataModule(cell_line=train_line,\n",
    "                  data_win_radius=4,\n",
    "                  batch_size=1,\n",
    "                  label_type='gamma')\n",
    "    dm.setup()\n",
    "    \n",
    "    early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val weighted mse loss\",\n",
    "    min_delta=0.00,\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode='min')\n",
    "    \n",
    "    rootdir = \"Experiments/Transformer_Cross_Cell_Line_Comparison\"\n",
    "    if not os.path.isdir(rootdir):\n",
    "        os.mkdir(rootdir)\n",
    "    logger = DictLogger(trial,\n",
    "                       rootdir)\n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        gpus=1,\n",
    "        max_epochs=200,\n",
    "        default_root_dir=rootdir,\n",
    "        callbacks=[early_stop_callback]\n",
    "    )\n",
    "    model = TransformerModule(\n",
    "        ntoken=1,\n",
    "        ninp=29,\n",
    "        nhid=2048,\n",
    "        nhead=1,\n",
    "        nlayers=5,\n",
    "        dropout=0.2,\n",
    "        optimi=\"SGD\",\n",
    "        lr=0.0001)\n",
    "    trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "all_metrics = {}\n",
    "for j, test_line in enumerate(cell_lines):\n",
    "    dm = FlyDataModule(cell_line=train_line,\n",
    "                  data_win_radius=4,\n",
    "                  batch_size=1,\n",
    "                  label_type='gamma')\n",
    "    dm.setup()\n",
    "    for i, train_line in enumerate(cell_lines):\n",
    "        layer_weights = glob.glob(\"Experiments/Transformer_Cross_Cell_Line_Comparison/optuna/version_\"+str(i)+\"/checkpoints/*\")[0]\n",
    "        layer_model   = TransformerModule.load_from_checkpoint(layer_weights).to(\"cuda:0\")\n",
    "        metrics = getModelMetrics(layer_model, dm, 'test')\n",
    "        all_metrics[i,j]=metrics\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getmetric = lambda met, i,j: all_metrics[i,j][met]\n",
    "print(getmetric('r2',0,0))\n",
    "\n",
    "\n",
    "for met in ['mse','mae','r2','pearson','spearman']:\n",
    "    vals = np.zeros((3,3))\n",
    "    for i in range(0,3):\n",
    "        for j in range(0,3):\n",
    "            vals[i,j]=\"{:.2f}\".format(getmetric(met, i,j))\n",
    "    print(met)\n",
    "    fig, ax = plt.subplots(1, figsize=(10,10))\n",
    "    table = ax.table(vals,\n",
    "             cellLoc=\"center\",\n",
    "            colLabels=cell_lines,\n",
    "            rowLabels=cell_lines)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.5, 1.5)\n",
    "    ax.axis('off')\n",
    "    plt.subplots_adjust(left=0.2, top=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different TAD characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
