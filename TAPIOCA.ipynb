{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HoneyDew\n",
    "Here we present Honeydew, a network for predicting genomic structural features using epigenetic data.  In keeping with the naming convention of \"muppet networks\" honeydew is a tool which incorporates self-attention mechanisms to effectively predict higher order genomic conformational structures such as AB compartments and TADs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Assembly\n",
    "First we gather some data for our experimetns.  We first pull epigenetic data from Roswnwald et al \"A machine learning framework for the prediction of chromatin folding in drosophila using epigenetic features\" https://peerj.com/articles/cs-307/\n",
    "\n",
    "This data contains a collection of 18 epigentic track features along three different cell lines and corresponding amaratus gamma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir Data\n",
    "!mkdir Data/Drosophilla\n",
    "!wget -P Data/Drosophilla https://raw.githubusercontent.com/MichalRozenwald/Hi-ChIP-ML/master/data/epigenetics/s2_kc_bg_scaled_18_features_2901.csv\n",
    "!wget -P Data/Drosophilla https://raw.githubusercontent.com/MichalRozenwald/Hi-ChIP-ML/master/data/target/s2_kc_bg_clean_gamma_2901.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this data briefly using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_feat = pd.read_csv(\"Data/Drosophilla/s2_kc_bg_scaled_18_features_2901.csv\",\n",
    "                sep=',')\n",
    "print(\"Label:\", str(df_feat.columns))\n",
    "df_label  = pd.read_csv(\"Data/Drosophilla/s2_kc_bg_clean_gamma_2901.csv\", \n",
    "                 sep=',')\n",
    "print(\"Features:\", str(df_label.columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that fetures dataframe currently has a column called 'gamma',\n",
    "this is the gamma value described inside of the armatus paper.\n",
    "*https://almob.biomedcentral.com/articles/10.1186/1748-7188-9-14*\n",
    "\n",
    "While we could work with just gamma we are also interested in checking if epigenetic data can be used to predict other meaningfull downstream metrics such as insulation score and ab compartment vector.  Thus before running experiments we would like to extend the dataset to encompass some of these datasets.  To do this will require Identification of labels of these metrics using Hi-C Data.  Thus we need to download the raw Hi-C Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir Data/Drosophilla/HiC_Maps\n",
    "!wget -P Data/Drosophilla/HiC_Maps ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE69nnn/GSE69013/suppl/GSE69013_BG3_merged_IC-heatmap-20K.txt.gz\n",
    "!wget -P Data/Drosophilla/HiC_Maps ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE69nnn/GSE69013/suppl/GSE69013_KC_merged_IC-heatmap-20K.txt.gz\n",
    "!wget -P Data/Drosophilla/HiC_Maps ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE69nnn/GSE69013/suppl/GSE69013_S2_merged_IC-heatmap-20K.txt.gz\n",
    "!gunzip Data/Drosophilla/HiC_Maps/GSE69013_BG3_merged_IC-heatmap-20K.txt.gz\n",
    "!gunzip Data/Drosophilla/HiC_Maps/GSE69013_KC_merged_IC-heatmap-20K.txt.gz\n",
    "!gunzip Data/Drosophilla/HiC_Maps/GSE69013_S2_merged_IC-heatmap-20K.txt.gz\n",
    "\n",
    "cell_line_hics = [\n",
    "    \"Data/Drosophilla/HiC_Maps/GSE69013_S2_merged_IC-heatmap-20K.txt\",\n",
    "    \"Data/Drosophilla/HiC_Maps/GSE69013_KC_merged_IC-heatmap-20K.txt\",\n",
    "    \"Data/Drosophilla/HiC_Maps/GSE69013_BG3_merged_IC-heatmap-20K.txt\",\n",
    "]\n",
    "\n",
    "cell_lines = ['S2','KC','BG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!mkdir Other_Tools/Juicer\n",
    "#!git clone https://github.com/aidenlab/juicer.git Other_Tools/Juicer\n",
    "#!mkdir Data/Drosophilla/Fastq\n",
    "\n",
    "#!prefetch SRR2032292\n",
    "#!fastq-dump SRR2032292 --split-files -O Data/Drosophilla/Fastq\n",
    "#!wget -P Data/Drosophilla http://igenomes.illumina.com.s3-website-us-east-1.amazonaws.com/Drosophila_melanogaster/UCSC/dm3/Drosophila_melanogaster_UCSC_dm3.tar.gz \n",
    "#!tar -zxvf Data/Drosophilla/Drosophila_melanogaster_UCSC_dm3.tar.gz -C Data/Drosophilla\n",
    "!bwa mem -SP5M -t 8 Data/Drosophilla/Drosophila_melanogaster/UCSC/dm3/Sequence/BWAIndex/genome.fa Data/Drosophilla/Fastq/SRR2032292_1.fastq  Data/Drosophilla/Fastq/SRR2032292_2.fastq  > test.sam\n",
    "\n",
    "# TODO Remove chimeric reads\n",
    "# TODO Sort\n",
    "# TODO remove dups\n",
    "# TODO juicer pre\n",
    "# extract eigen\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls Data/Drosophilla/Fastq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAD \n",
    "\n",
    "Using the downloaded Hi-C data we will extract chromosomal contact matrices from which TAD metrics can be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line_hics = [\n",
    "    \"Data/Drosophilla/HiC_Maps/GSE69013_S2_merged_IC-heatmap-20K.txt\",\n",
    "    \"Data/Drosophilla/HiC_Maps/GSE69013_KC_merged_IC-heatmap-20K.txt\",\n",
    "    \"Data/Drosophilla/HiC_Maps/GSE69013_BG3_merged_IC-heatmap-20K.txt\",\n",
    "]\n",
    "\n",
    "cell_lines = ['S2','KC','BG']\n",
    "chro_mats = {}\n",
    "\n",
    "for cell_line, fn in zip(cell_lines, cell_line_hics):\n",
    "    hic  = np.loadtxt(fn,\n",
    "          dtype=str,\n",
    "             skiprows=1)\n",
    "    mat   = hic[:,1:].astype(float)\n",
    "    bins  = hic[:,0]\n",
    "    np.save(\"bins.npy\", bins)\n",
    "    def getChro(x):\n",
    "        return x.split(\":\")[0]\n",
    "    chro_at_bins = np.array(list(map(getChro, bins)))\n",
    "    chros     = np.unique(chro_at_bins)\n",
    "    chros     = np.delete(chros, 4)# the epigentic data is not used for chro 4\n",
    "    for chro in chros:\n",
    "        d1_mask = chro_at_bins != chro\n",
    "        chro_mats[cell_line, chro] = np.delete(np.delete(mat,\n",
    "                            d1_mask,\n",
    "                           axis=0), d1_mask,\n",
    "                           axis=1)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insulation Scores\n",
    "We determine insulation scores using the Hi-C data, to do this we firt build  helping funcition comupte Insualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class computeInsulation(torch.nn.Module):\n",
    "    def __init__(self, window_radius=10, deriv_size=10):\n",
    "        super(computeInsulation, self).__init__()\n",
    "        self.window_radius = window_radius\n",
    "        self.deriv_size  = deriv_size\n",
    "        self.di_pool     = torch.nn.AvgPool2d(kernel_size=(2*window_radius+1), stride=1) #51\n",
    "        self.top_pool    = torch.nn.AvgPool1d(kernel_size=deriv_size, stride=1)\n",
    "        self.bottom_pool = torch.nn.AvgPool1d(kernel_size=deriv_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        iv     = self.di_pool(x)\n",
    "        iv     = torch.diagonal(iv, dim1=2, dim2=3)\n",
    "        iv     = torch.log2(iv/torch.mean(iv))\n",
    "        top    = self.top_pool(iv[:,:,self.deriv_size:])\n",
    "        bottom = self.bottom_pool(iv[:,:,:-self.deriv_size])\n",
    "        dv     = (top-bottom)\n",
    "        left   = torch.cat([torch.zeros(dv.shape[0], dv.shape[1],2), dv], dim=2)\n",
    "        right  = torch.cat([dv, torch.zeros(dv.shape[0], dv.shape[1],2)], dim=2)\n",
    "        band   = ((left<0) == torch.ones_like(left)) * ((right>0) == torch.ones_like(right))\n",
    "        band   = band[:,:,2:-2]\n",
    "        boundaries = []\n",
    "        for i in range(0, band.shape[0]):\n",
    "            cur_bound = torch.where(band[i,0])[0]+self.window_radius+self.deriv_size\n",
    "            boundaries.append(cur_bound)\n",
    "        return iv, dv, boundaries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply this insulation using a few different diagonal radius to capture different ranges of tad sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line_hics = [\n",
    "    \"Data/Drosophilla/HiC_Maps/GSE69013_S2_merged_IC-heatmap-20K.txt\",\n",
    "    \"Data/Drosophilla/HiC_Maps/GSE69013_KC_merged_IC-heatmap-20K.txt\",\n",
    "    \"Data/Drosophilla/HiC_Maps/GSE69013_BG3_merged_IC-heatmap-20K.txt\",\n",
    "]\n",
    "\n",
    "cell_lines = ['S2','KC','BG']\n",
    "win_radii  = [3,5,10]\n",
    "insul_vecs = {}\n",
    "dv_vecs    = {}\n",
    "\n",
    "for win_radius in win_radii:\n",
    "    insulationComputer = computeInsulation(window_radius=win_radius)\n",
    "    for cell_line, fn in zip(cell_lines, cell_line_hics):\n",
    "        for chro in chros:\n",
    "            mat_torch = torch.unsqueeze(torch.unsqueeze(torch.from_numpy(chro_mats[cell_line, chro]),\n",
    "                                                    dim=0), dim=1)\n",
    "            iv, dv, boundaries = insulationComputer(mat_torch.to(dtype=torch.float32))\n",
    "            insul_vecs[cell_line, win_radius, chro] = iv[0,0,:].numpy()\n",
    "            dv_vecs[cell_line, win_radius, chro]    = dv[0,0,:].numpy() #*2+4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dv_vecs['S2',10, '2L'].shape)\n",
    "print(insul_vecs['S2', 10, '2L'].shape)\n",
    "dv_vecs['S2',5,'2L'][0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have computed insulation scores for the chromosomes we add them to our label df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win_radius in win_radii:\n",
    "    insulation_column = []\n",
    "    dv_column         = []\n",
    "    for cl in cell_lines:\n",
    "        for chro in chros:\n",
    "            window_buff = np.repeat(np.nan, (win_radius-1))\n",
    "            insulation_column.extend(np.repeat(np.nan, (win_radius)))\n",
    "            insulation_column.extend(insul_vecs[cl, win_radius, chro])\n",
    "            insulation_column.extend(np.repeat(np.nan, (win_radius-1)))\n",
    "            \n",
    "            dv_column.extend(np.repeat(np.nan, (win_radius+10-1)))\n",
    "            dv_column.extend(dv_vecs[cl, win_radius, chro])\n",
    "            dv_column.extend(np.repeat(np.nan, (win_radius+10-1)))\n",
    "            \n",
    "    #df_label['insulation_'+str(win_radius)] = insulation_column \n",
    "    #df_label['difference_'+str(win_radius)] = dv_column\n",
    "    df_label['insulation_'+str(win_radius)] = dv_column\n",
    "print(df_label.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directionality Index\n",
    "firs we build a helper class for computing directionality index as described in Dixon et al 2012 https://www.nature.com/articles/nature11082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "class computeDirectionality(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 radius=2\n",
    "                ):\n",
    "        self.up   = torch.zeros((2*radius+1, 2*radius+1))\n",
    "        self.down = torch.zeros((2*radius+1, 2*radius+1))\n",
    "        self.down[radius+1:,radius]    = 1\n",
    "        self.up[:radius, radius]       = 1\n",
    "        self.up   = torch.unsqueeze(self.up, 0)\n",
    "        self.up   = torch.unsqueeze(self.up, 0)\n",
    "        self.down = torch.unsqueeze(self.down, 0)\n",
    "        self.down = torch.unsqueeze(self.down, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a       = F.conv2d(x, self.up)\n",
    "        b       = F.conv2d(x, self.down)\n",
    "        e       = (a+b)/2\n",
    "        sign    =  torch.sign(b-a)\n",
    "        term    = ((((a-e)**2)/e)+(((b-e)**2)/e))\n",
    "        di      = sign * term\n",
    "        di      = di.squeeze()\n",
    "        di_vec  = torch.diagonal(di)\n",
    "        return di_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply this class to the different cell lines using a few different possible parameters for window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_vecs = {}\n",
    "for win_radius in win_radii:\n",
    "    directionalityComputer = computeDirectionality(radius=win_radius)\n",
    "    for cell_line, fn in zip(cell_lines, cell_line_hics):\n",
    "        for chro in chros:\n",
    "            mat_torch = torch.unsqueeze(torch.unsqueeze(torch.from_numpy(chro_mats[cell_line, chro]),\n",
    "                                                    dim=0), dim=1)\n",
    "            di_vec = directionalityComputer.forward(mat_torch.to(dtype=torch.float32))\n",
    "            direction_vecs[cell_line, win_radius, chro] = di_vec.numpy() #/200+3\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add these direciton vecs to the label object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win_radius in win_radii:\n",
    "    direction_column = []\n",
    "    for cl in cell_lines:\n",
    "        for chro in chros:\n",
    "            window_buff = np.repeat(np.nan, (win_radius-1))\n",
    "            direction_column.extend(np.repeat(np.nan, (win_radius)))\n",
    "            direction_column.extend(direction_vecs[cl, win_radius, chro])\n",
    "            direction_column.extend(np.repeat(np.nan, (win_radius-1)))\n",
    "    df_label['directionality_'+str(win_radius)] = direction_column \n",
    "print(df_label.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the TAD Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rows_to_drop = df_label.isnull().any(axis=1)\n",
    "#clean_feature_df = df_feat[~rows_to_drop.to_numpy()]\n",
    "#clean_labels_df  = df_label[~rows_to_drop.to_numpy()]\n",
    "#print(clean_labels_df.columns)\n",
    "#print(clean_feature_df.columns)\n",
    "\n",
    "clean_features_df = df_feat[ ~df_label.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "clean_labels_df   = df_label[~df_label.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "\n",
    "print(clean_features_df.shape)\n",
    "print(clean_labels_df.shape)\n",
    "clean_labels_df.to_csv(\"Data/Drosophilla/clean_labels.csv\")\n",
    "clean_features_df.to_csv(\"Data/Drosophilla/clean_features.csv\")\n",
    "\n",
    "#alt_clean_labels = df_label[~df_label.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "#print(alt_clean_labels)\n",
    "#print(clean_labels_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loader\n",
    "Now that we have labels assembled for a variety of TAD identifying metrics we use pytorch lightning datamodule to create a dataloader object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#TODO CHECK THIS\n",
    "INSULATION_30 = 9\n",
    "\n",
    "class FlyDataModule(LightningDataModule):\n",
    "    class FlyDataset(Dataset):\n",
    "        def _get_cell_line_idx(self, strin):\n",
    "            if strin == \"S2\":\n",
    "                return 2\n",
    "            if strin ==\"KC\":\n",
    "                return 3\n",
    "            if string == \"BG\":\n",
    "                return 4\n",
    "        \n",
    "        def _get_label_col(self, label_type, label_val):\n",
    "            if label_type == \"gamma\":\n",
    "                return 7\n",
    "            if label_type == \"insulation\":\n",
    "                if label_val==10:\n",
    "                    return 8\n",
    "                if label_val==20:\n",
    "                    return 9\n",
    "                if label_val==30:\n",
    "                    return 10\n",
    "            if label_type ==\"directionality\":\n",
    "                if label_val ==10:\n",
    "                    return 11\n",
    "                if label_val ==20:\n",
    "                    return 12\n",
    "                if label_val ==30:\n",
    "                    return 13\n",
    "                \n",
    "            \n",
    "        def __init__(self,\n",
    "                    cell_line,\n",
    "                    tvt,\n",
    "                    data_win_radius,\n",
    "                    label_type,\n",
    "                    label_val):\n",
    "            self.cell_line       = cell_line\n",
    "            self.data_win_radius = data_win_radius\n",
    "            self.tvt             = tvt\n",
    "            self.label_type      = label_type\n",
    "            self.label_val       = label_val\n",
    "            self.feature_vecs = []\n",
    "            self.label_vecs   = []\n",
    "        \n",
    "            FEATURE_STRING = \"Data/Drosophilla/clean_features.csv\"\n",
    "            LABEL_STRING   = \"Data/Drosophilla/clean_labels.csv\"\n",
    "            \n",
    "            \n",
    "            cell_line_idx = self._get_cell_line_idx(cell_line)\n",
    "        \n",
    "            features       = np.loadtxt(FEATURE_STRING,\n",
    "                                   delimiter=',',\n",
    "                                   dtype=str)\n",
    "        \n",
    "            labels         = np.loadtxt(LABEL_STRING,\n",
    "                                   delimiter=',',\n",
    "                                   dtype=str)\n",
    "            \n",
    "            self.feature_head = features[0]\n",
    "            self.label_head   = labels[0]\n",
    "                        \n",
    "            features          = features[1:]\n",
    "            labels            = labels[1:]\n",
    "        \n",
    "            features          = features[features[:, cell_line_idx].astype(int)==1]\n",
    "            labels            = labels[labels[:, cell_line_idx].astype(int)==1]\n",
    "            features          = features[:,7:].astype(float)\n",
    "            \n",
    "            \n",
    "            label_idx         = self._get_label_col(self.label_type, \n",
    "                                                    self.label_val)\n",
    "            self.labels       = labels[:, label_idx].astype(float)\n",
    "            \n",
    "            \n",
    "            #if label_type == \"insulation\" and label_val==30:\n",
    "            #    self.labels = labels[:,INSULATION_30].astype(float)\n",
    "            \n",
    "            \n",
    "            self.features = preprocessing.scale(features,\n",
    "                                           axis=0,\n",
    "                                           with_mean=True,\n",
    "                                           with_std=True)\n",
    "            \n",
    "            feature_vecs = []\n",
    "            label_vecs   = []\n",
    "            \n",
    "            for i in range(0, self.features.shape[0] - self.data_win_radius):\n",
    "                start = i - self.data_win_radius\n",
    "                end   = i + self.data_win_radius + 1\n",
    "                if start < 0:\n",
    "                    continue\n",
    "                feature_vec = self.features[start:end]\n",
    "                label_vec   = self.labels[start:end]\n",
    "                feature_vecs.append(feature_vec)\n",
    "                label_vecs.append(label_vec)\n",
    "                \n",
    "            self.feature_vecs = np.array(feature_vecs)\n",
    "            self.label_vecs   = np.expand_dims(np.array(label_vecs), axis=2)\n",
    "            \n",
    "            if self.tvt == 'mini':\n",
    "                self.feature_vecs = self.feature_vecs[0:100]\n",
    "                self.label_vecs   = self.label_vecs[0:100]\n",
    "                \n",
    "            if self.tvt == 'train':\n",
    "                cutoff=int(.7*self.feature_vecs.shape[0])\n",
    "                self.feature_vecs = self.feature_vecs[:cutoff]\n",
    "                self.label_vecs   = self.label_vecs[:cutoff]\n",
    "                \n",
    "            if self.tvt == 'val':\n",
    "                cutoff1=int(.7*self.feature_vecs.shape[0])\n",
    "                cutoff2=int(.85*self.feature_vecs.shape[0])\n",
    "                self.feature_vecs = self.feature_vecs[cutoff1:cutoff2]\n",
    "                self.label_vecs   = self.label_vecs[cutoff1:cutoff2]\n",
    "                \n",
    "            if self.tvt == 'test':\n",
    "                cutoff=int(.85*self.feature_vecs.shape[0])\n",
    "                self.feature_vecs = self.feature_vecs[cutoff:]\n",
    "                self.label_vecs   = self.label_vecs[cutoff:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.feature_vecs.shape[0]\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return self.feature_vecs[idx], self.label_vecs[idx]\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 cell_line,\n",
    "                 data_win_radius,\n",
    "                 batch_size,\n",
    "                 label_type,\n",
    "                 label_val='na'):\n",
    "        super().__init__()\n",
    "        self.batch_size      = batch_size\n",
    "        self.cell_line       = cell_line\n",
    "        self.data_win_radius = data_win_radius\n",
    "        self.label_type      = label_type\n",
    "        self.label_val       = label_val\n",
    "    \n",
    "    def setup(self):\n",
    "        self.train = self.FlyDataset(cell_line=self.cell_line,\n",
    "                        tvt=\"train\",\n",
    "                        data_win_radius=self.data_win_radius,\n",
    "                        label_type=self.label_type,\n",
    "                        label_val=self.label_val)\n",
    "        \n",
    "        self.val   = self.FlyDataset(cell_line=self.cell_line,\n",
    "                        tvt=\"val\",\n",
    "                        data_win_radius=self.data_win_radius,\n",
    "                        label_type=self.label_type,\n",
    "                        label_val=self.label_val)\n",
    "        \n",
    "        self.test  = self.FlyDataset(cell_line=self.cell_line,\n",
    "                        tvt=\"test\",\n",
    "                        data_win_radius=self.data_win_radius,\n",
    "                        label_type=self.label_type,\n",
    "                        label_val=self.label_val)\n",
    "                                \n",
    "        \n",
    "        print(\"Everything set\")\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, \n",
    "                          batch_size=self.batch_size,\n",
    "                         num_workers=8)\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val,\n",
    "                          batch_size=self.batch_size,\n",
    "                         num_workers=8)\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test,\n",
    "                        batch_size=self.batch_size,\n",
    "                         num_workers=8)\n",
    "    \n",
    "    \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dm = FlyDataModule(cell_line=\"S2\",\n",
    "                  data_win_radius=10,\n",
    "                  batch_size=4,\n",
    "                  label_type=\"insulation\",\n",
    "                  label_val=10)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b, batch in enumerate(dm.train_dataloader()):\n",
    "    feature, label = batch\n",
    "    feature = feature.float()\n",
    "    label   = label.float()\n",
    "    fig, ax = plt.subplots(2)\n",
    "    print(feature.shape)\n",
    "    ax[0].imshow(feature[0], vmin=0, vmax=5)\n",
    "    ax[1].plot(label[0])\n",
    "    ax[1].set_ylim(-.05,0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "Now we show a few of the models that we are interested in.  The paper from which we derived our epigenetic dataset\n",
    "https://peerj.com/articles/cs-307/ used a bidirection LSTMModel as well as regression and gradient boosting.  We expand the list of models to test to include:\n",
    "    Gated Recurrent Units,\n",
    "    Elman Recurrent Neural Netorks.\n",
    "    \n",
    "We also develop honeydew, a transformer based network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\".\")\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import pdb\n",
    "from torch import nn\n",
    "from torch.nn import RNN\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class RNNModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "            lr=1e-5,\n",
    "            input_size=29,\n",
    "            hidden_size=1,\n",
    "            num_layers=1,\n",
    "            dropout=0,\n",
    "            bidirectional=False,\n",
    "            optimi=torch.optim.Adam\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.dropout=dropout\n",
    "        self.bidirectional=bidirectional\n",
    "        self.optimi=optimi\n",
    "        self.rnn = nn.RNN(input_size=self.input_size,\n",
    "                        hidden_size=self.hidden_size,\n",
    "                        num_layers=self.num_layers,\n",
    "                        nonlinearity='relu',\n",
    "                        bias=True,\n",
    "                        batch_first=True,\n",
    "                        dropout=self.dropout,\n",
    "                        bidirectional=self.bidirectional)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hn = self.rnn(x)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        feature, label = batch\n",
    "        feature        = feature.float()\n",
    "        label          = label.float()\n",
    "        output, hn     = self.rnn(feature)\n",
    "        loss           = F.mse_loss(output, label)\n",
    "        self.log(\"mse_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        if self.optimi == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        if self.optimi == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        if self.optimi == \"RMSprop\":\n",
    "            optimizer = torch.optim.RMSprop(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Transformer\n",
    "from torch import nn\n",
    "\n",
    "class  TransformerModule(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        print(\"init\")\n",
    "        self.model = nn.Transformer(nhead=16,\n",
    "                                    num_encoder_layers=12)\n",
    "    def forward(self, x):\n",
    "        print(\"forward\")\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        feature, label = batch\n",
    "        feature        = feature.float()\n",
    "        label          = label.float()\n",
    "        out            = transformer_model()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO this is debuging adjusy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model   = RNNModule(\n",
    "    lr=.0001)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,\n",
    "                     max_epochs=50)\n",
    "\n",
    "dm      = FlyDataModule(cell_line=\"S2\",\n",
    "                        batch_size=4,\n",
    "                        data_win_radius=5,\n",
    "                        label_type='gamma',\n",
    "                        label_val=10)\n",
    "dm.setup()\n",
    "trainer.fit(model, dm)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "## Evaluation\n",
    "Using all features to predict gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import yaml\n",
    "\n",
    "dm = FlyDataModule(cell_line=\"S2\",\n",
    "                 data_win_radius=5,\n",
    "                 batch_size=1,\n",
    "                 label_type='gamma',\n",
    "                 label_val=10)\n",
    "dm.setup()\n",
    "dataloader = dm.test_dataloader()\n",
    "VERSION    = 0\n",
    "PATH       = glob.glob(\"lightning_logs/version_\"+str(VERSION)+\"/checkpoints/*\")[0]\n",
    "op         = open(\"lightning_logs/version_\"+str(VERSION)+\"/hparams.yaml\")\n",
    "param      = yaml.load(op)\n",
    "model      = RNNModule()\n",
    "pretrained_model = model.load_from_checkpoint(PATH)\n",
    "pretrained_model.freeze()\n",
    "for b, batch in enumerate(dm.test_dataloader()):\n",
    "    feature, label = batch\n",
    "    feature = feature.float()\n",
    "    label   = label.float()\n",
    "    output   = pretrained_model(feature)\n",
    "    if b == 50:\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.plot(label[0,:,0].detach().numpy(),\n",
    "                c=\"blue\",\n",
    "               label=\"label\")\n",
    "        ax.plot(output[0,:,0].detach().numpy(),\n",
    "               c=\"green\",\n",
    "               label=\"prediction\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we need a custom logging mechanis,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulled from https://github.com/optuna/optuna/issues/1186\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "from pytorch_lightning.loggers import LightningLoggerBase\n",
    "\n",
    "class DictLogger(LightningLoggerBase):\n",
    "    \"\"\"PyTorch Lightning `dict` logger.\"\"\"\n",
    "\n",
    "    def __init__(self, version, root_dir):\n",
    "        super(DictLogger, self).__init__()\n",
    "        self.metrics = []\n",
    "        self._version = version\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def log_metrics(self, metrics, step=None):\n",
    "        self.metrics.append(metrics)\n",
    "\n",
    "    @property\n",
    "    def version(self):\n",
    "        return self._version\n",
    "\n",
    "    @property\n",
    "    def experiment(self):\n",
    "        \"\"\"Return the experiment object associated with this logger.\"\"\"\n",
    "\n",
    "    def log_hyperparams(self, params):\n",
    "        \"\"\"\n",
    "        Record hyperparameters.\n",
    "        Args:\n",
    "            params: :class:`~argparse.Namespace` containing the hyperparameters\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(self.root_dir):\n",
    "            os.mkdir(self.root_dir)\n",
    "        if not os.path.isdir(self.root_dir+\"/optuna\"):\n",
    "            os.mkdir(self.root_dir+\"/optuna\")\n",
    "        dirr = self.root_dir+'/optuna/version_'+str(self._version)\n",
    "        if not os.path.isdir(dirr):\n",
    "            os.mkdir(self.root_dir+'/optuna/version_'+str(self._version))\n",
    "        with open(dirr+\"/hparams.yaml\", 'w') as outfile:\n",
    "            print(\"logging them hyperparams:\"+str(self.root_dir))\n",
    "            yaml.dump(params, outfile)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"Return the experiment name.\"\"\"\n",
    "        return 'optuna'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "Train each model architecture to predict gamma.\n",
    "### todo for gamma prediction we follow the suggestion of https://peerj.com/articles/cs-307/ and use a loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import yaml\n",
    "import os\n",
    "import optuna\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "dm = FlyDataModule(cell_line=\"S2\",\n",
    "                 data_win_radius=5,\n",
    "                 batch_size=1,\n",
    "                 label_type='gamma')\n",
    "dm.setup()\n",
    "\n",
    "model_classes = [RNNModule]\n",
    "for model_class in model_classes:\n",
    "    def objective(trial):\n",
    "        root_dir = \"Experiments/Experiment_1_Gamma\"\n",
    "        if not os.path.isdir(\"Experiments\"):\n",
    "            os.mkdir(\"Experiments\")\n",
    "        if not os.path.isdir(root_dir):\n",
    "            os.mkdir(root_dir)\n",
    "        logger  = DictLogger(trial.number,\n",
    "                            root_dir)\n",
    "        trainer = pl.Trainer(\n",
    "            logger=logger,\n",
    "            gpus=1,\n",
    "            max_epochs=10,\n",
    "            default_root_dir=root_dir\n",
    "        )\n",
    "        \n",
    "        #hyperparameters\n",
    "        input_size=29\n",
    "        lr         = trial.suggest_categorical(\"lr\", [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n",
    "        num_layers = trial.suggest_categorical(\"num_layers\", [1, 4, 8, 16, 32, 64, 128, 256, 512])\n",
    "        optimi     = trial.suggest_categorical(\"optimi\", [\"Adam\", \"SGD\"])\n",
    "        model = model_class(lr=lr,\n",
    "                           input_size=input_size,\n",
    "                           num_layers=num_layers,\n",
    "                           optimi=optimi)\n",
    "        trainer.fit(model, dm)\n",
    "        return logger.metrics[-1]['mse_loss']\n",
    "\n",
    "        \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=2)\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "    print(\"Best trial\")\n",
    "    trial = study.best_trial\n",
    "    print(trial)\n",
    "    print(\"value:{}\".format(trial.value))\n",
    "    print(\" params:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"  {}: {}\".format(key, value))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize results of Experiment 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_WIN_RADIUS = 5\n",
    "\n",
    "\n",
    "dm = FlyDataModule(cell_line=\"S2\",\n",
    "                 data_win_radius=DATA_WIN_RADIUS,\n",
    "                 batch_size=1,\n",
    "                 label_type='gamma',\n",
    "                 label_val=10)\n",
    "dm.setup()\n",
    "dataloader = dm.test_dataloader()\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(20,20))\n",
    "\n",
    "for VERSION in list(range(0,2)):\n",
    "    PATH       = glob.glob(\"Experiments/Experiment_1_Gamma/optuna/version_\"+str(VERSION)+\"/checkpoints/*\")[0]\n",
    "    op         = open(\"Experiments/Experiment_1_Gamma/optuna/version_\"+str(VERSION)+\"/hparams.yaml\")\n",
    "    param      = yaml.load(op)\n",
    "    model      = RNNModule()\n",
    "    pretrained_model = model.load_from_checkpoint(PATH)\n",
    "    pretrained_model.freeze()\n",
    "    \n",
    "    \n",
    "    full_output = []\n",
    "    full_label  = []\n",
    "    for b, batch in enumerate(dm.test_dataloader()):\n",
    "        feature, label = batch\n",
    "        feature  = feature.float()\n",
    "        label    = label.float()\n",
    "        output   = pretrained_model(feature)\n",
    "        if b %(2*DATA_WIN_RADIUS+1)==0:\n",
    "            full_output.extend(output[0,:,0].detach().numpy())\n",
    "            full_label.extend(label[0,:,0].detach().numpy())\n",
    "            \n",
    "    if b > 1:\n",
    "        ax.plot(full_output,\n",
    "            label=\"prediction\"+str(VERSION),\n",
    "            linewidth=5)\n",
    "    \n",
    "    if VERSION == 0:\n",
    "       ax.plot(full_label,\n",
    "             label=\"label\",\n",
    "             linewidth=5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
