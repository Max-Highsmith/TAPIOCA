{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import optuna\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from Data.Drosophilla.FlyDataMod import FlyDataModule\n",
    "from IPython.display import Image\n",
    "from IPython.core.debugger import set_trace\n",
    "from Models import BiLSTM as bl\n",
    "from Models import LinearRegression as lrg\n",
    "from Models import Transformer as tfm\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from sklearn import preprocessing\n",
    "from torch import nn as nn\n",
    "from Utils import evaluations as ev\n",
    "from Utils import callbacks as cb\n",
    "from Utils import loggers as lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_val=10\n",
    "label_type=\"gamma\"\n",
    "dm    = FlyDataModule(cell_line=\"S2\",\n",
    "                  data_win_radius=5,\n",
    "                  batch_size=1,\n",
    "                  label_type=label_type,\n",
    "                  label_val=label_val)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression\n",
    "!mkdir Experiments/Benchmark_Architectures/Regression\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=50,\n",
    "    callbacks=[cb.getcb()],\n",
    "    default_root_dir=\"Experiments/Benchmark_Architectures/Regression\",\n",
    "    terminate_on_nan=True)\n",
    "\n",
    "model_linear = lrg.LinearRegressionModule(\n",
    "    inputSize=29,\n",
    "    outputSize=1,\n",
    "    hasRidge=True)\n",
    "model_linear.cuda()\n",
    "#trainer.fit(model_linear, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bilstm\n",
    "!mkdir Experiments/Benchmark_Architectures/LSTM\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=50,\n",
    "    callbacks=[cb.getcb()],\n",
    "    default_root_dir=\"Experiments/Benchmark_Architectures/LSTM\",\n",
    "    terminate_on_nan=True)\n",
    "\n",
    "model_lstm =  bl.BiLSTMModule(\n",
    "            input_size=29,\n",
    "            hidden_size=64,\n",
    "            num_layers=5,\n",
    "            lr=0.0001,\n",
    "            dropout=0.2)\n",
    "model_lstm.cuda()\n",
    "#trainer.fit(model_lstm, dm)\n",
    "print(\"done trianing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer \n",
    "#bilstm\n",
    "!mkdir Experiments/Benchmark_Architectures/Transformer\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=50,\n",
    "    callbacks=[cb.getcb()],\n",
    "    default_root_dir=\"Experiments/Benchmark_Architectures/Transformer\",\n",
    "    terminate_on_nan=True)\n",
    "\n",
    "model_tfm  = tfm.TransformerModule(\n",
    "                ntoken=1,\n",
    "                ninp=29,\n",
    "                nhead=1,\n",
    "                nhid=2048,\n",
    "                nlayers=1,\n",
    "                dropout=0,\n",
    "                optimi=\"Adam\",\n",
    "                lr=.001)\n",
    "model_tfm.cuda()\n",
    "#trainer.fit(model_tfm, dm)\n",
    "#print(\"done training Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import evaluations as eva\n",
    "\n",
    "model_linear.cuda()\n",
    "linear_path = glob.glob(\"Experiments/Benchmark_Architectures/Regression/lightning_logs/version_0/checkpoints/*\")[0]\n",
    "model_linear = model_linear.load_from_checkpoint(linear_path).cuda()\n",
    "print(eva.getModelMetrics(model_linear, dm, 'test'))\n",
    "ev.createPlot(model_linear, dm, 'test', 'morgarbage.png')\n",
    "\n",
    "model_lstm.cuda()\n",
    "lstm_path = glob.glob(\"Experiments/Benchmark_Architectures/LSTM/lightning_logs/version_0/checkpoints/*\")[0]\n",
    "model_lstm = model_lstm.load_from_checkpoint(lstm_path).cuda()\n",
    "print(eva.getModelMetrics(model_lstm, dm, 'test'))\n",
    "ev.createPlot(model_lstm, dm, \"test\", \"garbage.png\")\n",
    "\n",
    "model_tfm.cuda()\n",
    "tfm_path = glob.glob(\"Experiments/Benchmark_Architectures/Transformer/lightning_logs/version_0/checkpoints/*\")[0]\n",
    "model_tfm = model_tfm.load_from_checkpoint(tfm_path).cuda()\n",
    "print(eva.getModelMetrics(model_tfm, dm, 'test'))\n",
    "ev.createPlot(model_tfm, dm, 'test', 'moregarbage.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
